参考来源：
https://juejin.cn/post/7037047570400542727?searchId=2025060419034981468EDF016930796DBC
1.基础概念
  首先，什么是机器学习？机器学习约等于找这样一个函数，比如在语音识别中，输入一段语音，输出文字内容，在图像识别中，输入一张图像，输出图中的对象，在围棋中，输入棋盘数据，输出下一步怎么走，在对话系统中，输入一句 hi ，输出一句回应，而这个函数，是由你写的程序加上大量的数据，然后由机器自己学习到的。
  怎么找这样一个函数呢，让我们从线性模型入手。线性模型形式简单，易于建模，但是蕴含着机器学习中一些重要的基本思想，许多功能更为强大的非线性模型都可在线性模型的基础上通过引入层级结构或高维映射而得到。
2.线性模型
  我们以一个猫和狗的分类来看，我们在教一个小朋友区分猫和狗的时候，并不会给到一个维奇百科的定义，而是不断的让小朋友看到猫和狗，让他判断，然后告诉他正确答案，纠结错误认知。机器学习也是同理，不断告知计算机怎样是正确的，纠正计算机的认知，不同的是，小朋友的认知是人脑自动处理完成的，而计算机并不能自动的构建猫和狗的记忆，计算机只认识数字。
  所以我们需要提取出代表猫和狗的特征，然后用数字来表示。为了简化例子，我们这里只用到两个特征，鼻子的大小以及耳朵的形状，一般来说猫猫的鼻子更小，耳朵更尖，而狗狗鼻子比较大，耳朵比较圆。
  我们对多张图片，统计图片中耳朵以及鼻子特征，在一个二维坐标中表现出来，可以看到猫猫和狗狗会分布在坐标系的不同区域。
  肉眼可见，我们可以用一条直线来区分，但是，计算机并看不到哪里可以画条线。如何将信息传递给计算机呢，让我们定义两个变量，x1 表示鼻子大小，x2 表示耳朵形状，再定义这样一个直线方程 W1 · X1 + W2 · X2 - b = 0，也就相当于，令y=W1 · X1 + W2 · X2 - b ，当 y 大于0，判断是猫，当 y 小于 0 ，判断是狗。
  现在，从计算机的角度来看，它拥有了一堆数据，
  以及一个线性模型。
  还差一个目标/任务，我们的期望是，当给一个没有见过的 x ，通过 f(x) ，可以得到一个预测值 y ，这个 y 要能够尽可能的贴近真实的值，这样，就有了一台有用的萌宠分类机了！这样的目标如何用数字来表示呢，这就要引入一个概念损失函数(Loss function)了，损失函数计算的是预测值与真实值之间的差距。
  常用的损失函数有绝对值损失函数(Absolute value loss),也就是两个数值差的绝对值，就很直观，距离目标差多少，加起来，就酱
  还有平方损失函数(最小二乘法, Least squares loss)
  平方损失函数的目标是让每个点到回归直线的距离最小，这个距离算的是欧几里得距离。 现在，我们给计算机的目标就变成了求一个最小值，
  为了求这个值，让我们回忆一下久违的微积分，（同样，为了简化到二维坐标系，假设只有一个需要求的 w ），导数为 0 的地方即是函数的极大值或者极小值。
  对于图中这样一个简单的一元二次方程，我们可以直接对参数 w 求导，求得极小值。但是，如果是下图中这样一个函数呢，就..不好求了，而且对于不同的函数求导有不同的公式，那就..比较麻烦了，毕竟我们的目标是让机器自己学习，是吧。
  所以，我们需要一个更通用的计算方法，那就是梯度下降(Gradient descent， 梯度下降的基本流程如下，首先，我们随机取一个点作为初始值，计算这个点的斜率，也就是导数。
  当斜率为负的时候，往右走一小步。
  当斜率为正的时候，往左走一小步。
  在每个点上重复，计算新的斜率，再适当的走一小步，就会逼近函数的某个局部最小值，就像一个小球从山上滚下来，不过初始位置不同，会到达不同的局部最小值，无法保证是全局最小，但是，其实，大部分情况我们根据问题抽象的函数基本都是凸函数，能够得到一个极小值，在极小值不唯一的情况下，也可以加入随机数，来给到一个跳出当前极小值区域的机会。我们需要明确的是，机器学习的理论支撑是概率论与统计学，我们通过机器学习寻找的问题答案，往往不是最优解，而是一个极优解。
  想象一个更复杂的有两个输入一个输出的二元函数，我们的 loss function 可以呈现为三维空间中的一个曲面，问题就变成了，曲面上某个点要往空间中哪个方向走，才能让结果下降得最快。
  步骤依旧是，计算梯度，更新，计算，更新....用公示来表示就是如下。
  这时候，我们就遇到了第一个超参数 η ，即学习率(Learning rate)，机器学习中的参数分为两类，模型参数与超参数，模型参数是 w 这种，让机器自己去学习的，超参数则是在模型训练之前由开发人员指定的。
  通过上面的公式，可以看到。
  是 Loss function 函数对于参数 w 的导数，决定了我们走的方向，那么学习率则决定了在这个方向每一小部走的距离。
  当 η 太小，到达极小值的过程会非常的缓慢，而如果 η 太大，则会因为步伐太大，直接越过最低点。那么，η 的值要怎么取呢。
  比较常规的做法是，以从 0.1 这样的值开始，然后再指数下降，取0.01，0.001，当我们用一个很大的学习率，会发现损失函数的值几乎没有下降，那可能就是在摇摆，当我们取到一个较小的值，能够让损失函数下降，那么继续往下取，不断缩小范围，这个过程也可以通过计算机自动来做，如果有计算资源的话。
  了解了梯度下降、学习率后，我们已经可以使用线性模型解决比较简单的问题了。
  基本步骤：
  提取特征
  设定模型
  计算梯度，更新
3.多层感知机
  我们刚刚说到的线性模型，实际上是一个单层的网络，它包括了机器学习的基本要素，模型、训练数据、损失函数和优化算法。但是受限于线性运算，并不能解决更加复杂的问题。
  我们需要更为通用的模型来适应不同的数据。比如多加一层？加一层的效果约等于对坐标轴进行变换，可以做更复杂一丢丢的问题了。
  但是依旧是线性模型，没有办法解决非线性问题，比如下图中，没有办法用一条直线分开，但是用 y= x2 这样一个二元一次方程就可以轻轻松松，这就是非线性的好处了。
  加一个非线性的结构，也就引入了神经网络中另一个基本概念，激活函数(Activation Function)，常见的激活函数如下
  
  
  
